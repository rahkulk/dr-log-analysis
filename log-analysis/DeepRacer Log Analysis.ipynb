{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation Run Log Analysis and Visualization for AWS DeepRacer\n",
    "\n",
    "This notebook walks through how you can analyze and debug using the AWS DeepRacer Simulation logs \n",
    "\n",
    "\n",
    "1. Tools to find best iteration of your model\n",
    "1. Visualize reward distribution on the track\n",
    " - Visualize reward heatmap per episode or iteration\n",
    "1. Identify hotspots on the track for your model\n",
    "1. Understand probability distributions on simulated images\n",
    "1. Evaluation run analysis - plot lap speed heatmap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "boto3 >= 1.9.133  ; configure your aws cli and/or boto credentials file\n",
    "\n",
    "AWS CLI: https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html\n",
    "\n",
    "Boto Configuration: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IAM permissions\n",
    "\n",
    "\n",
    "Assign your Sagemaker notebook an execution role with permission to access the deepracer service. Typically this is done by providing \"deepracer:*\" permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import tarfile\n",
    "import requests\n",
    "import json\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shapely Library\n",
    "from shapely.geometry import Point, Polygon\n",
    "from shapely.geometry.polygon import LinearRing, LineString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from log_analysis import *\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jobs run from AWS DeepRacer Console. Download the desired log file by providing model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"Sample\" ## Change to your model\n",
    "is_training = True  ## Make this False if you want to do log analysis on Evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./tmp\n",
    "!rm -rf ./intermediate_checkpoint\n",
    "!rm -rf ./downloaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envroot = os.getcwd()\n",
    "aws_data_path = set(os.environ.get('AWS_DATA_PATH', '').split(os.pathsep))\n",
    "aws_data_path.add(os.path.join(envroot, 'models'))\n",
    "os.environ.update({'AWS_DATA_PATH': os.pathsep.join(aws_data_path)})\n",
    "\n",
    "region = \"us-east-1\"\n",
    "dr_client = boto3.client('deepracer', region_name=region,\n",
    "        endpoint_url=\"https://deepracer-prod.{}.amazonaws.com\".format(region))\n",
    "models = dr_client.list_models(ModelType=\"REINFORCEMENT_LEARNING\",MaxResults=100)[\"Models\"]\n",
    "for model in models:\n",
    "    if model[\"ModelName\"]==model_name:\n",
    "        break\n",
    "ModelArn=model[\"ModelArn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_training:\n",
    "    training_job=dr_client.list_training_jobs(ModelArn=ModelArn,MaxResults=100)[\"TrainingJobs\"][0]\n",
    "    training_log_url= dr_client.get_asset_url(Arn=training_job['JobArn'], AssetType=\"LOGS\")['Url']\n",
    "\n",
    "    with requests.get(training_log_url, stream=True) as response:\n",
    "        with open(\"{}.tar.gz\".format(model_name), \"wb\") as tarball:\n",
    "            for chunk in response.iter_content(16384):\n",
    "                tarball.write(chunk)\n",
    "else:       \n",
    "    evaluation_job = dr_client.list_evaluations(ModelArn=ModelArn,MaxResults=100)[\"EvaluationJobs\"][0]\n",
    "    evaluation_log_url= dr_client.get_asset_url(Arn=evaluation_job['JobArn'], AssetType=\"LOGS\")['Url']\n",
    "\n",
    "    with requests.get(evaluation_log_url, stream=True) as response:\n",
    "        with open(\"{}.tar.gz\".format(model_name), \"wb\") as tarball:\n",
    "            for chunk in response.iter_content(16384):\n",
    "                tarball.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelUuid=ModelArn[ModelArn.rfind('/')+1:]\n",
    "simtrace_path = \"./downloaded_model/{}/sim-trace/training/training-simtrace/\".format(ModelUuid)\n",
    "if not is_training:\n",
    "    simtrace_path = \"./downloaded_model/{}/sim-trace/evaluation/*/evaluation-simtrace/\".format(ModelUuid)\n",
    "merged_simtrace_path = \"./logs/deepracer-{}.csv\".format(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p downloaded_model/{ModelUuid}\n",
    "!mkdir -p intermediate_checkpoint/{ModelUuid}/model-artifacts\n",
    "\n",
    "!tar -xf {model_name}.tar.gz -C ./downloaded_model/\n",
    "!mkdir -p ./tmp\n",
    "!rsync -a --delete --include=*.csv --exclude=* {simtrace_path} ./tmp/\n",
    "!rm -rf downloaded_model/{model_name}\n",
    "!rm -rf {model_name}.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_url=dr_client.get_asset_url(Arn=model[\"ModelArn\"],AssetType=\"COMPLETE_MODEL_ARTIFACT\")['Url']\n",
    "    \n",
    "with requests.get(model_url, stream=True) as response:\n",
    "    with open(\"{}-model.tar.gz\".format(model_name), \"wb\") as tarball:\n",
    "        for chunk in response.iter_content(16384):\n",
    "            tarball.write(chunk)\n",
    "\n",
    "!tar zxvf {model_name}-model.tar.gz -C intermediate_checkpoint/ \\*.csv {ModelUuid}/model/* {ModelUuid}/metrics/*\n",
    "!rm -rf {model_name}-model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_url= dr_client.get_asset_url(Arn=model[\"ModelArn\"], AssetType=\"MODEL_ARTIFACT\")['Url']\n",
    "\n",
    "with requests.get(model_url, stream=True) as response:\n",
    "    with open(\"{}-model-artifacts.tar.gz\".format(model_name), \"wb\") as tarball:\n",
    "        for chunk in response.iter_content(16384):\n",
    "            tarball.write(chunk)\n",
    "\n",
    "!tar zxvf {model_name}-model-artifacts.tar.gz -C intermediate_checkpoint/{ModelUuid}/model-artifacts\n",
    "!rm -rf {model_name}-model-artifacts.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"intermediate_checkpoint/{}/model/model_metadata.json\".format(ModelUuid),\"r\") as jsonin:\n",
    "    model_metadata=json.load(jsonin)\n",
    "sensor = [sensor for sensor in model_metadata['sensor'] if sensor != \"LIDAR\"][0]\n",
    "model_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job run from Public Notebook. Download the desired log file given the simulation ID "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge all the csv files into one big .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sort_csv_file():\n",
    "    sim_trace_csvs = glob.glob(\"./tmp/*.csv\")\n",
    "    csvs_with_ids = [(int(os.path.basename(file).split(\"-\")[0]), file) for file in sim_trace_csvs]\n",
    "    csvs_sorted = sorted(csvs_with_ids, key=lambda csvs_with_ids: csvs_with_ids[0])\n",
    "    return [csv_file[1] for csv_file in csvs_sorted]\n",
    "    \n",
    "def merge_csv_files(output_file_path):\n",
    "    csv_files = get_sort_csv_file()\n",
    "    header_saved = False\n",
    "    with open(output_file_path, 'w') as fout:\n",
    "        for csv_file in csv_files:\n",
    "            with open(csv_file) as fin:\n",
    "                header = next(fin)\n",
    "                if not header_saved:\n",
    "                    fout.write(header)\n",
    "                    header_saved = True\n",
    "                for line in fin:\n",
    "                    line = re.sub(r'(\\[[^\\]]*\\])', r'\"\\1\"', line, flags=re.M)\n",
    "                    fout.write(line)\n",
    "\n",
    "merge_csv_files(merged_simtrace_path)\n",
    "!tail -n 3 $merged_simtrace_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load waypoints for the track you want to run analysis on\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ListFiles = [f for f in listdir(\"tracks/\") if isfile(join(\"tracks/\", f))]\n",
    "print(ListFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_track_waypoints(track_name):\n",
    "    return np.load(\"tracks/%s.npy\" % track_name)\n",
    "\n",
    "if is_training:\n",
    "    track_arn=training_job[\"Config\"][\"TrackConfig\"][\"TrackArn\"]\n",
    "else:\n",
    "    track_arn=evaluation_job[\"Config\"][\"TrackArn\"]\n",
    "\n",
    "trackname=track_arn[track_arn.rfind(\"/\")+1:]\n",
    "waypoints = get_track_waypoints(trackname)\n",
    "waypoints.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Track and Waypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_points(ax, points):\n",
    "    ax.scatter(points[:-1,0], points[:-1,1], s=1)\n",
    "    for i,p in enumerate(points):\n",
    "        ax.annotate(i, (p[0], p[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "plot_points(ax, waypoints[:-1,0:2])\n",
    "plot_points(ax, waypoints[:-1,2:4])\n",
    "plot_points(ax, waypoints[:-1,4:6])\n",
    "ax.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_center_line = LineString(waypoints[:,0:2])\n",
    "l_inner_border = LineString(waypoints[:,2:4])\n",
    "l_outer_border = LineString(waypoints[:,4:6])\n",
    "road_poly = Polygon(np.vstack((l_outer_border, np.flipud(l_inner_border))))\n",
    "\n",
    "road_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale waypoints to centimeter scale\n",
    "\n",
    "center_line = waypoints[:,0:2] \n",
    "inner_border = waypoints[:,2:4]\n",
    "outer_border = waypoints[:,4:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_track(df, track_size=(500, 800), x_offset=0, y_offset=0, scale=100):\n",
    "    '''\n",
    "    Each track may have a diff track size, \n",
    "    For reinvent track, use track_size=(500, 800)\n",
    "    Tokyo, track_size=(700, 1000)\n",
    "    x_offset, y_offset is used to convert to the 0,0 coordinate system\n",
    "    '''\n",
    "    #track = np.zeros(track_size) # lets magnify the track by *100\n",
    "    #for index, row in df.iterrows():\n",
    "    #    x = int(row[\"x\"]*scale + x_offset)\n",
    "    #    y = int(row[\"y\"]*scale + y_offset)\n",
    "    #    reward = row[\"reward\"]\n",
    "    #    track[y, x] = reward\n",
    "    #   plt.plot(y,x,reward)\n",
    "    allx=[]\n",
    "    ally=[]\n",
    "    allreward=[]\n",
    "    for index, row in df.iterrows():\n",
    "        x = float(row[\"x\"])\n",
    "        y = float(row[\"y\"])\n",
    "        reward = float(row[\"reward\"])\n",
    "        allx.append(x)\n",
    "        ally.append(y)\n",
    "        allrewards=reward\n",
    "        #track[y, x] = reward\n",
    "        #plt.plot(y,x,reward)\n",
    "    heatmap, xedges, yedges = np.histogram2d(allx, ally, bins=50)\n",
    "    extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]\n",
    "        \n",
    "    #fig = plt.figure(1, figsize=(12, 16))\n",
    "    fig = plt.figure(1, figsize=track_size)\n",
    "    ax = fig.add_subplot(111)\n",
    "    print_border(ax, center_line, inner_border, outer_border)\n",
    "    return heatmap.T,extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_laps(sorted_idx, n_laps=5):\n",
    "    fig = plt.figure(n_laps, figsize=(12, 30))\n",
    "    for i in range(n_laps):\n",
    "        idx = sorted_idx[i]\n",
    "        \n",
    "        episode_data = episode_map[idx]\n",
    "        \n",
    "        ax = fig.add_subplot(n_laps,1,i+1)\n",
    "        \n",
    "        line = LineString(center_line)\n",
    "        plot_coords(ax, line)\n",
    "        plot_line(ax, line)\n",
    "        \n",
    "        line = LineString(inner_border)\n",
    "        plot_coords(ax, line)\n",
    "        plot_line(ax, line)\n",
    "\n",
    "        line = LineString(outer_border)\n",
    "        plot_coords(ax, line)\n",
    "        plot_line(ax, line)\n",
    "\n",
    "\n",
    "        for idx in range(1, len(episode_data)-1):\n",
    "            x1,y1,action,reward,angle,speed = episode_data[idx]\n",
    "            car_x2, car_y2 = x1 - 0.02, y1\n",
    "            plt.plot([x1, car_x2], [y1, car_y2], 'b.')\n",
    "        \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODE_PER_ITER = 20 if is_training else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(merged_simtrace_path)\n",
    "iteration_arr = np.arange(math.ceil(df.episode.max()/EPISODE_PER_ITER)+1) * EPISODE_PER_ITER\n",
    "df['iteration'] = np.digitize(df.episode, iteration_arr)\n",
    "df = df.rename(columns={\"X\": \"x\", \"Y\": \"y\", \"tstamp\": \"timestamp\"})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model loaded uses a continuous action space, convert it to a discrete action space for analysis. This will map the choices the model made in the simtrace logs into discrete buckets to allow for later visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ANGLE_BUCKETS = 5\n",
    "NUM_SPEED_BUCKETS = 4\n",
    "\n",
    "if 'action_space_type' in model_metadata and model_metadata['action_space_type']=='continuous':\n",
    "    max_angle = model_metadata['action_space']['steering_angle']['high']\n",
    "    min_angle = model_metadata['action_space']['steering_angle']['low']\n",
    "\n",
    "    max_speed = model_metadata['action_space']['speed']['high']\n",
    "    min_speed = model_metadata['action_space']['speed']['low']\n",
    "\n",
    "    #Determine which discrete bucket would be the equivalent for the continuous action space\n",
    "    for index, row in df.iterrows():        \n",
    "        angle_bucket = math.floor(((row[\"steer\"] - min_angle)/(max_angle-min_angle))*NUM_ANGLE_BUCKETS)\n",
    "        speed_bucket = math.floor(((row[\"throttle\"] - min_speed)/(max_speed-min_speed))*NUM_SPEED_BUCKETS)\n",
    "        if angle_bucket==NUM_ANGLE_BUCKETS:\n",
    "            angle_bucket -= 1\n",
    "        if speed_bucket==NUM_SPEED_BUCKETS:\n",
    "            speed_bucket -= 1\n",
    "        df.at[index,\"action\"] = int(angle_bucket*NUM_SPEED_BUCKETS+speed_bucket)\n",
    "    \n",
    "\n",
    "    #Convert the model metadata in memory to use the new forced discrete action space\n",
    "    angle_bucket_size = (max_angle-min_angle)/NUM_ANGLE_BUCKETS\n",
    "    angle = min_angle+.5*angle_bucket_size\n",
    "    speed_bucket_size = (max_speed-min_speed)/NUM_SPEED_BUCKETS\n",
    "    speed = min_speed+.5*speed_bucket_size    \n",
    "    model_metadata['action_space'] = []\n",
    "    index = 0\n",
    "    for anglei in range(0,NUM_ANGLE_BUCKETS):\n",
    "        for speedi in range(0,NUM_SPEED_BUCKETS):\n",
    "            model_metadata['action_space'].append({'index':index,\n",
    "                                 'speed': speed,\n",
    "                                 'steering_angle': angle})\n",
    "            index+=1\n",
    "            speed += speed_bucket_size\n",
    "        angle += angle_bucket_size\n",
    "        speed = min_speed+.5*speed_bucket_size\n",
    "            \n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['y'].min(), df['x'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the rewards to a 0-1 scale\n",
    "\n",
    "from sklearn.preprocessing import  MinMaxScaler\n",
    "min_max_scaler = MinMaxScaler()\n",
    "scaled_vals = min_max_scaler.fit_transform(df['reward'].values.reshape(df['reward'].values.shape[0], 1))\n",
    "df['reward'] = pd.DataFrame(scaled_vals.squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reward'].min(), df['reward'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot rewards per Iteration\n",
    "\n",
    "This graph is useful to understand the mean reward and standard deviation within each episode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REWARD_THRESHOLD = 0\n",
    "\n",
    "# reward graph per episode\n",
    "min_episodes = np.min(df['episode'])\n",
    "max_episodes = np.max(df['episode'])\n",
    "print('Number of episodes = ', max_episodes)\n",
    "\n",
    "total_reward_per_episode = list()\n",
    "    \n",
    "for epi in np.arange(min_episodes, max_episodes+1,1):\n",
    "    df_slice = df[df['episode'] == epi]\n",
    "    total_reward_per_episode.append(np.sum(df_slice['reward']))\n",
    "\n",
    "average_reward_per_iteration = list()\n",
    "deviation_reward_per_iteration = list()\n",
    "\n",
    "buffer_rew = list()\n",
    "for val in total_reward_per_episode:\n",
    "    buffer_rew.append(val)\n",
    "\n",
    "    if len(buffer_rew) == EPISODE_PER_ITER:\n",
    "        average_reward_per_iteration.append(np.mean(buffer_rew))\n",
    "        deviation_reward_per_iteration.append(np.std(buffer_rew))\n",
    "        # reset\n",
    "        buffer_rew = list()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(6, 12))\n",
    "ax = fig.add_subplot(311)\n",
    "ax.plot(np.arange(len(average_reward_per_iteration)), average_reward_per_iteration, '.')\n",
    "ax.set_title('Rewards per Iteration')\n",
    "ax.set_ylabel('Mean reward')\n",
    "ax.set_xlabel('Iteration')\n",
    "\n",
    "for rr in range(len(average_reward_per_iteration)):\n",
    "    if average_reward_per_iteration[rr] >= REWARD_THRESHOLD :\n",
    "        ax.plot(rr, average_reward_per_iteration[rr], 'r.')\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "ax = fig.add_subplot(312)\n",
    "ax.plot(np.arange(len(deviation_reward_per_iteration)), deviation_reward_per_iteration, '.')\n",
    "\n",
    "ax.set_ylabel('Dev of reward')\n",
    "ax.set_xlabel('Iteration')\n",
    "plt.grid(True)\n",
    "\n",
    "for rr in range(len(average_reward_per_iteration)):\n",
    "    if average_reward_per_iteration[rr] >= REWARD_THRESHOLD:\n",
    "        ax.plot(rr, deviation_reward_per_iteration[rr], 'r.')\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(313)\n",
    "ax.plot(np.arange(len(total_reward_per_episode)), total_reward_per_episode, '.')\n",
    "ax.set_ylabel('Total reward')\n",
    "ax.set_xlabel('Episode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze training/evaluation metrics with progress and reward function\n",
    "\n",
    "This graph gives you an idea whether your model has convergered or more training is required. If you see the curve trending upwards then more training time would help the agent to get better rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_training:\n",
    "    METRIC_PATH = glob.glob(\"./intermediate_checkpoint/{}/metrics/training/*.json\".format(ModelUuid))[0]\n",
    "else:\n",
    "    METRIC_PATH = glob.glob(\"./intermediate_checkpoint/{}/metrics/evaluation/*.json\".format(ModelUuid))[0]\n",
    "    \n",
    "with open(METRIC_PATH, \"r\") as fp:\n",
    "    data = json.loads(fp.read())\n",
    "    metric_data = data['metrics']\n",
    "    df_metrics = pd.DataFrame(metric_data)\n",
    "\n",
    "if is_training:\n",
    "    df_metrics = df_metrics[df_metrics['phase'] == \"training\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins= [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "\n",
    "df_metrics = df_metrics.sort_values(by=\"metric_time\")\n",
    "print('Mean percentage: {}'.format(df_metrics.completion_percentage.mean()))\n",
    "\n",
    "completion_percentage_np = np.array(df_metrics.completion_percentage)\n",
    "episode_progress_buckets = np.split(completion_percentage_np[:EPISODE_PER_ITER*(len(completion_percentage_np)//EPISODE_PER_ITER)],\n",
    "                                    len(completion_percentage_np)//EPISODE_PER_ITER)\n",
    "episode_progress_mean = np.mean(episode_progress_buckets, axis=1)\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "# Line graph showing the mean iteration of completion progress\n",
    "ax1.plot(episode_progress_mean)\n",
    "ax1.title.set_text('Iteration Mean completion progress')\n",
    "ax1.set_xlabel('Number of iterations')\n",
    "ax1.set_ylabel('Percentage completion')\n",
    "\n",
    "# Bar chart to show completion_percentage with bucketing of 10% bar chart\n",
    "ax2.hist(df_metrics.completion_percentage, bins=bins, edgecolor=\"k\")\n",
    "ax2.title.set_text('Bucket cout of completion percentage')\n",
    "ax2.set_xlabel('Lap completion bins 10 units')\n",
    "ax2.set_ylabel('Episode counts')\n",
    "\n",
    "# Line graph showing the mean iteration of reward mean\n",
    "if is_training:\n",
    "    reward_score_np = np.array(df_metrics.reward_score)\n",
    "    episode_reward_buckets = np.split(reward_score_np[:EPISODE_PER_ITER*(len(reward_score_np)//EPISODE_PER_ITER)],\n",
    "                                                      len(reward_score_np)//EPISODE_PER_ITER)\n",
    "    episode_reward_mean = np.mean(episode_reward_buckets, axis=1)\n",
    "    ax3.plot(episode_reward_mean)\n",
    "    plt.title(\"Metric analysis - Reward/percentage vs number of iterations\")\n",
    "    ax3.title.set_text('Iteration Mean reward')\n",
    "    ax3.set_xlabel('Number of iterations')\n",
    "    ax3.set_ylabel('Reward score')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the reward distribution for your reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Min x-axis {}; Max x-axis {}\".format(np.min(df['x']), np.max(df['x'])))\n",
    "print(\"Min y-axis {}; Max y-axis {}\".format(np.min(df['y']), np.max(df['y'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track,extent = plot_track(df, track_size=(12, 12), x_offset=0, y_offset=0)\n",
    "plt.title(\"Reward distribution for all actions \")\n",
    "im = plt.imshow(track, cmap='hot', extent=extent, interpolation='bilinear', origin=\"lower\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a particular iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_id = 1\n",
    "track,extent = plot_track(df[df['iteration'] == iteration_id], track_size=(12, 12), x_offset=0, y_offset=0)\n",
    "plt.title(\"Reward distribution for all actions \")\n",
    "im = plt.imshow(track, cmap='hot', extent=extent, interpolation='bilinear', origin=\"lower\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path taken for top reward iterations\n",
    "\n",
    "NOTE: in a single episode, the car can go around multiple laps, the episode is terminated when car completes 1000 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_map, episode_map, sorted_idx = episode_parser(df)    \n",
    "fig = plot_top_laps(sorted_idx[:], 3)\n",
    "print(\"The top 3 highest reward episodes are {}\".format(sorted_idx[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path taken in a particular episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation RUN\n",
    "def plot_episode_run(df, E):\n",
    "    fig = plt.figure(1, figsize=(12, 16))\n",
    "    ax = fig.add_subplot(211)\n",
    "    print_border(ax, center_line, inner_border, outer_border) \n",
    "    episode_data = df[df['episode'] == E]\n",
    "    for row in episode_data.iterrows():\n",
    "        x1,y1,action,reward = row[1]['x'], row[1]['y'], row[1]['action'], row[1]['reward']\n",
    "        car_x2, car_y2 = x1 - 0.02, y1\n",
    "        plt.plot([x1, car_x2], [y1, car_y2], 'r.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_episode_run(df, E=2) # arbitrary episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path taken in a particular Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_id = 1\n",
    "\n",
    "for i in range((iteration_id-1)*EPISODE_PER_ITER, (iteration_id)*EPISODE_PER_ITER):\n",
    "    plot_episode_run(df, E=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action breakdown per iteration and historgram for action distribution for each of the turns - reinvent track\n",
    "\n",
    "This plot is useful to understand the actions that the model takes for any given iteration.\n",
    "\n",
    "Say you want the car to go at higher speeds on the straight line. This will give you an idea what actions the car is taking along those segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The actions plotted on the tracks may become noise. Use this to clip the low reward values action.\n",
    "# Anything with reward < 0.8 is clipped. This is based on the reward function you trained on.\n",
    "\n",
    "REWARD_THRESHOLD = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track Segment Labels\n",
    "action_names = []\n",
    "for action in model_metadata['action_space']:\n",
    "    action_names.append(\"ST\"+str(action['steering_angle'])+\" SP\"+\"%.2f\"%action[\"speed\"])\n",
    "action_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define track labels**\n",
    "\n",
    "This hash defines the labels for track segments on various tracks. Analyzing new tracks will require adding a new entry to this hash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_segments_hash = { \"reinvent_base\": [(0, 'straightaway'),\n",
    "                                          (20, 'hairpin'),\n",
    "                                          (46, 'slight right'),\n",
    "                                          (61, 'left'),\n",
    "                                          (76, 'slight left'),\n",
    "                                          (90, 'straightaway'),\n",
    "                                          (103, 'left'),\n",
    "                                          (113, 'straightaway')                                         \n",
    "                                          ],\n",
    "                       \"reInvent2019_track\": [(0, 'left'),\n",
    "                                              (18, 'sharp right'),\n",
    "                                              (33, 'gentle left'),\n",
    "                                              (82, 'left'),\n",
    "                                              (93, 'slight left'),\n",
    "                                              (107, 'left'),\n",
    "                                              (117, 'right'),\n",
    "                                              (137, 'left')\n",
    "                                             ],\n",
    "                       \"arctic_open\": [(0, 'straightaway'),\n",
    "                                       (24,'left'),\n",
    "                                       (36,'right'),\n",
    "                                       (52,'left'),\n",
    "                                       (67,'hairpin left'),\n",
    "                                       (84,'right'),\n",
    "                                       (98,'slight left'),\n",
    "                                       (107,'straightaway'),\n",
    "                                       (125,'slight left'),\n",
    "                                       (134,'straightaway'),\n",
    "                                       (156,'hairpin left')            \n",
    "                       ],\n",
    "                       \"caecer_loop\": [(0, 'straightaway'),\n",
    "                                       (14,'slight left'),\n",
    "                                       (34,'straightaway'),\n",
    "                                       (42,'hairpin left'),\n",
    "                                       (70,'straightaway'),\n",
    "                                       (80,'sharp left'),\n",
    "                                       (93,'straightaway'),\n",
    "                                       (103,'slight left'),\n",
    "                                       (115,'straightaway')            \n",
    "                       ],\n",
    "                       \"red_star_open\":[(0,'straightaway'),\n",
    "                                        (29,'left'),\n",
    "                                        (41,'straightaway'),\n",
    "                                        (67,'hairpin left'),\n",
    "                                        (78,'straightaway'),\n",
    "                                        (94,'s-turn right'),\n",
    "                                        (107,'s-turn left'),\n",
    "                                        (119,'s-turn right'),\n",
    "                                        (130,'s-turn left'),\n",
    "                                        (140,'straightaway'),\n",
    "                                        (155,'sharp left'),\n",
    "                                        (163,'straightaway')                           \n",
    "                       ]\n",
    "}\n",
    "trackname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 4*len(action_names)))\n",
    "iterations_downselect = [iteration_id] ## Lets pick the iteratons with the highest rewards\n",
    "\n",
    "y_limit = 10\n",
    "track_segments = track_segments_hash[trackname]\n",
    "segment_x=[]\n",
    "segment_y=[]\n",
    "segment_xerr_l=[]\n",
    "segment_xerr_r=[]\n",
    "segment_yerr_n=[]\n",
    "segment_yerr_s=[]\n",
    "i=0\n",
    "while i<len(track_segments):\n",
    "    segment_x.append(track_segments[i][0])\n",
    "    segment_y.append(0)\n",
    "    segment_xerr_l.append(0)\n",
    "    if i>=len(track_segments)-1:\n",
    "        segment_xerr_r.append(len(waypoints)-track_segments[i][0])\n",
    "    else:\n",
    "        segment_xerr_r.append(track_segments[i+1][0]-track_segments[i][0])\n",
    "    segment_yerr_n.append(0)\n",
    "    segment_yerr_s.append(y_limit)\n",
    "    i+=2\n",
    "\n",
    "segment_x = np.array(segment_x)\n",
    "segment_y = np.array(segment_y)\n",
    "segment_xerr = np.array([segment_xerr_l,segment_xerr_r])\n",
    "segment_yerr = np.array([segment_yerr_n,segment_yerr_s])\n",
    "\n",
    "#segment_x = np.array(vert_lines)\n",
    "#segment_y = np.array([0]*len(vert_lines))\n",
    "#segment_xerr = np.array([[0]*len(vert_lines),[1]*len(vert_lines)])\n",
    "#segment_yerr = np.array([[0]*len(vert_lines), [150]*len(vert_lines)])\n",
    "\n",
    "wpts_array = center_line \n",
    "text_y=[.66*y_limit,.5*y_limit,.33*y_limit]\n",
    "    \n",
    "for iter_num in iterations_downselect:\n",
    "\n",
    "    # Slice the data frame to get all episodes in that iteration\n",
    "    df_iter = df[(iter_num == df['iteration'])]\n",
    "    n_steps_in_iter = len(df_iter)\n",
    "    print('Number of steps in iteration=', n_steps_in_iter)\n",
    "\n",
    "    # Reward function threshold\n",
    "    th = REWARD_THRESHOLD\n",
    "    for idx in range(len(action_names)):\n",
    "        ax = fig.add_subplot(len(action_names), 2, 2*idx+1)\n",
    "        print_border(ax, center_line, inner_border, outer_border) \n",
    "            \n",
    "        df_slice = df_iter[df_iter['reward'] >= th]\n",
    "        df_slice = df_slice[df_slice['action'] == idx]\n",
    "\n",
    "        ax.plot(df_slice['x'], df_slice['y'], 'b.')\n",
    "\n",
    "        for idWp in track_segments:\n",
    "            ax.text(wpts_array[idWp[0]][0], wpts_array[idWp[0]][1], str(idWp[0]), bbox=dict(facecolor='red', alpha=0.5))\n",
    "\n",
    "        #ax.set_title(str(log_name_id) + '-' + str(iter_num) + ' w rew >= '+str(th))\n",
    "        ax.set_ylabel(action_names[idx])\n",
    "\n",
    "        # calculate action way point distribution\n",
    "        action_waypoint_distribution = list()\n",
    "        for idWp in range(len(wpts_array)):\n",
    "            action_waypoint_distribution.append(len(df_slice[df_slice['closest_waypoint'] == idWp]))\n",
    "\n",
    "        ax = fig.add_subplot(len(action_names), 2, 2 * idx + 2)\n",
    "\n",
    "        # Call function to create error boxes\n",
    "        _ = make_error_boxes(ax, segment_x, segment_y, segment_xerr, segment_yerr)\n",
    "\n",
    "\n",
    "        i=0\n",
    "        for tt in range(len(track_segments)):\n",
    "            ax.text(track_segments[tt][0], text_y[i], track_segments[tt][1])\n",
    "            i = (i+1)%len(text_y)\n",
    "\n",
    "        ax.bar(np.arange(len(wpts_array)), action_waypoint_distribution)\n",
    "        ax.set_xlabel('waypoint')\n",
    "        ax.set_ylabel('# of actions')\n",
    "        ax.legend([action_names[idx]])\n",
    "        ax.set_ylim((0, y_limit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets analyze the actions chosen for each situation. Does this model choose to steer or go straight on straightaways? Does it choose to speed up or slow down? Are entire portions of the action space ignored, suggesting a mismatch between the action space at the reward function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation Image Analysis - Probability distribution on decisions (actions)\n",
    "\n",
    "is the model making decisions that are \"too close\" or is it confident for the laps it finishes. if the top and second best decisions are far apart, the model must likely be making more confident decisions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "img_path = \"simulation_episode/\"\n",
    "all_files = sorted(glob.glob(img_path + '/*.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download all the checkpoints (provided as an example).  \n",
    "We recommend downloading only the ones you are interested in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "from tensorflow.python.platform import gfile\n",
    "from PIL import Image\n",
    "\n",
    "GRAPH_PB_PATH = 'intermediate_checkpoint/'\n",
    "\n",
    "def load_session(pb_path):\n",
    "    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, \n",
    "                                    log_device_placement=True))\n",
    "    print(\"load graph:\", pb_path)\n",
    "    with gfile.FastGFile(pb_path,'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "    sess.graph.as_default()\n",
    "    tf.import_graph_def(graph_def, name='')\n",
    "    graph_nodes=[n for n in graph_def.node]\n",
    "    names = []\n",
    "    for t in graph_nodes:\n",
    "        names.append(t.name)\n",
    "    \n",
    "    # For front cameras/stereo camera use the below\n",
    "    x = sess.graph.get_tensor_by_name('main_level/agent/main/online/network_0/{}/{}:0'.format(sensor, sensor))\n",
    "    y = sess.graph.get_tensor_by_name('main_level/agent/main/online/network_1/ppo_head_0/policy:0')\n",
    "    \n",
    "    return sess, x, y\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inference = []\n",
    "iterations = [7,8,9]\n",
    "models_file_path = glob.glob(\"{}{}/model/model_*.pb\".format(GRAPH_PB_PATH, ModelUuid))\n",
    "\n",
    "for model_file in models_file_path:\n",
    "    model, obs, model_out = load_session(model_file)\n",
    "    arr = []\n",
    "    for f in all_files[:]:\n",
    "        img = Image.open(f)\n",
    "        img_arr = np.array(img)\n",
    "        img_arr = rgb2gray(img_arr)\n",
    "        img_arr = np.expand_dims(img_arr, axis=2)\n",
    "        current_state = {\"observation\": img_arr} #(1, 120, 160, 1)\n",
    "        y_output = model.run(model_out, feed_dict={obs:[img_arr]})[0]\n",
    "        arr.append (y_output)\n",
    "        \n",
    "    model_inference.append(arr)\n",
    "    model.close()\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_diff = []\n",
    "\n",
    "for model, model_file in zip(model_inference, models_file_path):\n",
    "    print(\"Inference for model: {}\".format(model_file))\n",
    "    for mi in model:\n",
    "        max1, max2 = mi.argsort()[-2:][::-1]\n",
    "        prob_diff.append(mi[max1] - mi[max2])\n",
    "    plt.hist(prob_diff)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model which appears to have a better seperation in probabability will work better in sim2real experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model CSV Analysis\n",
    "\n",
    "\n",
    "Download the model from the console AWS DeepRacer > Reinforcement learning > $Training Job Name$ > Download Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = './intermediate_checkpoint/{}/model-artifacts/worker_0.multi_agent_graph.main_level.main_level.agent_0.csv'.format(ModelUuid)\n",
    "df_csv = pd.read_csv(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Training\"\n",
    "df_csv.plot(x='Training Iter', y='Shaped Training Reward', style='.', \n",
    "        title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv['Episode Length'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the model looking at?\n",
    "\n",
    "Gradcam: visual heatmap of where the model is looking to make its decisions. based on https://arxiv.org/pdf/1610.02391.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "def visualize_gradcam_discrete_ppo(sess, rgb_img, category_index=0, num_of_actions=5):\n",
    "    '''\n",
    "    @inp: model session, RGB Image - np array, action_index, total number of actions \n",
    "    @return: overlayed heatmap\n",
    "    '''\n",
    "    \n",
    "    img_arr = np.array(img)\n",
    "    img_arr = rgb2gray(img_arr)\n",
    "    img_arr = np.expand_dims(img_arr, axis=2)\n",
    "    \n",
    "    x = sess.graph.get_tensor_by_name('main_level/agent/main/online/network_0/{}/{}:0'.format(sensor, sensor))\n",
    "    y = sess.graph.get_tensor_by_name('main_level/agent/main/online/network_1/ppo_head_0/policy:0')\n",
    "    feed_dict = {x:[img_arr]}\n",
    "\n",
    "    #Get he policy head for clipped ppo in coach\n",
    "    model_out_layer = sess.graph.get_tensor_by_name('main_level/agent/main/online/network_1/ppo_head_0/policy:0')\n",
    "    loss = tf.multiply(model_out_layer, tf.one_hot([category_index], num_of_actions))\n",
    "    reduced_loss = tf.reduce_sum(loss[0])\n",
    "    \n",
    "    # For front cameras use the below\n",
    "    conv_output = sess.graph.get_tensor_by_name('main_level/agent/main/online/network_1/{}/Conv2d_4/Conv2D:0'.format(sensor))\n",
    "    \n",
    "    grads = tf.gradients(reduced_loss, conv_output)[0]\n",
    "    output, grads_val = sess.run([conv_output, grads], feed_dict=feed_dict)\n",
    "    weights = np.mean(grads_val, axis=(1, 2))\n",
    "    cams = np.sum(weights * output, axis=3)\n",
    "\n",
    "    ##im_h, im_w = 120, 160##\n",
    "    im_h, im_w = rgb_img.shape[:2]\n",
    "\n",
    "    cam = cams[0] #img 0\n",
    "    image = np.uint8(rgb_img[:, :, ::-1] * 255.0) # RGB -> BGR\n",
    "    cam = cv2.resize(cam, (im_w, im_h)) # zoom heatmap\n",
    "    cam = np.maximum(cam, 0) # relu clip\n",
    "    heatmap = cam / np.max(cam) # normalize\n",
    "    cam = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET) # grayscale to color\n",
    "    cam = np.float32(cam) + np.float32(image) # overlay heatmap\n",
    "    cam = 255 * cam / (np.max(cam) + 1E-5) ##  Add expsilon for stability\n",
    "    cam = np.uint8(cam)[:, :, ::-1] # to RGB\n",
    "\n",
    "    return cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "img_path = \"simulation_episode/\"\n",
    "all_files = sorted(glob.glob(img_path + '/*.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_path = models_file_path[0] #Change this to your model 'pb' frozen graph file\n",
    "\n",
    "model, obs, model_out = load_session(model_path)\n",
    "heatmaps = []\n",
    "\n",
    "#Just need to match up the shape of the neural network\n",
    "if 'action_space_type' in model_metadata and model_metadata['action_space_type']=='continuous':\n",
    "    num_of_actions=2\n",
    "else:\n",
    "    num_of_actions=len(action_names)\n",
    "\n",
    "for f in all_files[:5]:\n",
    "    img = np.array(Image.open(f))\n",
    "    heatmap = visualize_gradcam_discrete_ppo(model, img, category_index=0, num_of_actions=num_of_actions)\n",
    "    heatmaps.append(heatmap)\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(heatmaps)):\n",
    "    plt.imshow(heatmaps[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p38",
   "language": "python",
   "name": "conda_tensorflow2_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
